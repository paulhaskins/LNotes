\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}

%\usepackage[tagged, highstructure]{accessibility}
\usepackage{tocloft}
\usepackage{arydshln}
\usetikzlibrary{arrows.meta, decorations.pathreplacing}
\usepackage{tikz-cd}
\usepackage{polynom}
\usepackage{pifont}
\newcommand{\pistar}{{\zf\symbol{"4A}}}
% a tiny helper for a stretched phantom (for the underbrace)
\newcommand\mc[1]{\multicolumn{1}{c}{#1}}


\newlist{steplist}{enumerate}{1}
\setlist[steplist,1]{%
  label=\underline{\textbf{\arabic*:}}, % gives “1:” “2:” … (bold, underlined)
  labelsep=0.6em,                      % space after the label
  leftmargin=2.8em,                    % indent so text lines up neatly
  itemsep=1\baselineskip,              % vertical gap between steps
  topsep=0pt,                          % no extra space before/after list
  align=left
}

\begin{document}
\title{Linear Algebra I}
\author{Lecture Notes Provided by Dr.~Miriam Logan.}
\date{}
\maketitle
\tableofcontents
\newpage  

 \section{Solving Linear Systems}
To solve a linear system, our goal will be to reduce an augmented matrix to row echelon form  or reduced row echelon form (using the row reduction algorithm) and then to solve the system.\\
\ex{}{
\textit{ Solve the following system of equations:}
\begin{align*}
  4y + z &= 2 \\
  3x +8y +z &= 12 \\
  x + 2y +z &= 2
.\end{align*}

Augmented matrixL
\[
\left[
\begin{array}{ccc;{2pt/2pt}c}  
  0 & 4 & 1 & 2\\
  3 & 8 & 1 & 12\\
  1 & 2 & 1 & 2\\
\end{array}
\right]    \xrightarrow[ r_1 \leftrightarrow r_3]{} \left[
\begin{array}{ccc;{2pt/2pt}c}  
  1 & 2 & 1 & 2\\
  3 & 8 & 1 & 12\\
  0 & 4 & 1 & 2\\
\end{array}
\right]
.\] 
\[
\xrightarrow[ r_2 - 3 r_1]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1  & 2  & 1  & 2\\
   0 & 2 & -2 & 6\\
   0 & 4  & 1 & 2 \\
 \end{array}
 \right]          \xrightarrow[ r_3 - 2 r_2]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1 & 2 & 1 & 2\\
   0 & 2 & -2 & 6\\
   0 & 0 & 5 & -10\\
 \end{array}
 \right]
.\] 
\[
\xrightarrow[ r_2 \times  \frac{1}{2}]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1 & 2 & 1 & 2\\
   0 & 1 & -1 & 3\\
   0 & 0 & 5 & -10\\
 \end{array}
 \right]          \xrightarrow[ r_3 \times \frac{1}{5}]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1 & 2 & 1 & 2\\
   0 & 1 & -1 & 3\\
   0 & 0 & 1 & -2\\
 \end{array}
 \right]
.\] 
\[
\xrightarrow[ r_2 + r_3]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1 & 2 & 1 & 2\\
   0 & 1 & 0 & 1\\
   0 & 0 & 1 & -2\\
 \end{array}
 \right]          \xrightarrow[ r_1 - r_3]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1 & 2 & 0 & 4\\
   0 & 1 & 0 & 1\\
   0 & 0 & 1 & -2\\
 \end{array}
 \right]
.\] 
\[
\xrightarrow[ r_1 - 2 r_2]{}
 \left[
 \begin{array}{ccc;{2pt/2pt}c}  
   1 & 0 & 0 & 2\\
   0 & 1 & 0 & 1\\
   0 & 0 & 1 & -2\\
 \end{array}
 \right]        \qquad x=2, \quad y=1, \quad z=-2
.\] 
}


\ex{}{
\textit{ Find the solution set of the following system of equations:}
\begin{align*}
  x + 2y &= 1 \\
  -x +y - z &= -2 \\
  5x + 16 y -2z &= 8
.\end{align*}
Augmented matrix:
\[
\left[
\begin{array}{ccc;{2pt/2pt}c}  
  1 & 2 & 0 & 1\\
  -1 & 1 & -1 & -2\\
  5 & 16 & -2  & 8\\
\end{array}
\right]           \xrightarrow[ r_2 + r_1]{ r_3 - 5 r_1} \left[
\begin{array}{ccc;{2pt/2pt}c}  
  1 & 2 & 0 & 1\\
  0 & 3 & -1 & -1\\
  0 & 6 & -2 & 3\\
\end{array}
\right]
.\] 
\[
\xrightarrow[ r_3 - 2 r_2]{} \left[
\begin{array}{ccc;{2pt/2pt}c}  
  1 & 2 & 0 & 1\\
  0 & 3 & -1 & -1\\
  0 & 0 & 0 & 5\\
\end{array}
\right]
.\] 
The last equation corresponds to $0=5$, which never happens. Hence the system is inconsistent, there are no solutions. The 3 planes in $  \mathbb{R} ^3$ do not intersect.\\
}

\ex{}{
 \textit{Find the solution set of the following system of equations:}
 \begin{align*}
   x+y&=1 \\
    2x+2y&=2 \\
 .\end{align*}
 \[
 \left[
 \begin{array}{cc;{2pt/2pt}c}  
    1& 1 & 1 \\
    2& 2 & 2 \\
 \end{array}
 \right]    \xrightarrow[ r_2 - 2 r_1]{}                \left[
 \begin{array}{cc;{2pt/2pt}c}  
\left[
 \begin{array}{ccccc;{2pt/2pt}c}
     &   &   &   &   & \
     &   &   &  &  & \
    &  &  &  &  & \
 \end{array}
 \right]    1& 1 & 1 \\
    0& 0 & 0 \\
 \end{array}
 \right]  
 .\] 
 All solutions to the linear system  satisfy $ x +y =1$ i.e.  all points that lie on the line $ x+y=1$ or $ y = 1-x$, $ x \in \mathbb{R}$, satisfy the system.\\
 \textbf{Note}
 \begin{enumerate}[label=(\arabic*).]  
   \item There exists an infinite number of solutions to the system.
   \item        The solutions are expressed in the form $ y = 1-x$, $ x \in \mathbb{R}$.\\
     We can express one dependent variable in terms of the independent (free) variable $ x$.
 \end{enumerate}
 
}


\ex{}{
    \textit{ Find the solution set of the following system of equations:}
    \\
    \begin{align*}
      x + 3y + 4z &= 2 \\
      -4x -11y - 15z &= -11 \\
      x + y + 2z &= 8
    .\end{align*}
    Augmented matrix:
    \[
    \left[
    \begin{array}{ccc;{2pt/2pt}c}  
      1 & 3 & 4 & 2\\
      -4 & -11 & -15 & -11\\
      1 & 1 & 2 & 8\\
    \end{array}
    \right] \xrightarrow[ r_2 + 4 r_1]{}
    \left[
    \begin{array}{ccc;{2pt/2pt}c}  
      1 & 3 & 4 & 2\\
      0 & 1 & 1 & -3\\
      1 & 1 & 2 & 8\\
    \end{array}
    \right]
    .\] 
    \[
    \xrightarrow[ r_3 -r_1]{}
    \left[
    \begin{array}{ccc;{2pt/2pt}c}  
      1 & 3 & 4 & 2\\
      0 & 1 & 1 & -3\\
      0 & -2 & -2 & 6\\
    \end{array}
    \right] \xrightarrow[ r_3 + 2 r_2]{}
     \left[
     \begin{array}{ccc;{2pt/2pt}c}  
       1 & 3 & 4 & 2\\
       0 & 1 & 1 & -3\\
       0 & 0 &0 &0\\
     \end{array}
     \right]
    .\] 
    \[
    \xrightarrow[ r_1 - 3 r_2]{}
    \left[
    \begin{array}{ccc;{2pt/2pt}c}  
      1 & 0 & 1 & 11\\
      0 & 1 & 1 & -3\\
      0 & 0 &0 &0\\
    \end{array}
    \right]
    .\] 
    Note no pivot in the $ z$ column\\
    \begin{align*}
      x + z &= 11 \\
      y + z &= -3\\
      z &=  \text{ (free variable)}
    .\end{align*}
    \begin{align*}
      x &= 11 - z \\
      y &= -3 - z\\
      z &= z \text{ (free variable)}, \qquad  z \in \mathbb{R}
    .\end{align*}
}

 \dfn{Pivot Position :}{
 A \underline{ Pivot position} in a matrix $ A$ is a position in $ A$ that has a leading one in the reduced row echelon form of $ A$.\\
 }
 \dfn{Pivot Column :}{
       A \underline{ Pivot column} in a matrix $ A$ is a column that contains a pivot position.\\
 }
 \dfn{Pivot Variable :}{
       A \underline{ Pivot variable} is a variable in a linear system whose corresponding column in the augmented matrix associated with the system is a pivot column.\\
 }
 \dfn{Free Variable :}{
 A variable that is not a pivot variable is called a \underline{free variable}.\\
 }
 \ex{}{
 Solve 
 \begin{align*}
   x_1 + 2 x_2 + x_3 +x_4 +x_5 &= 1 \\
   -3 x_1 -6 x_2 - 2 x_3 -x_5 &= -3 \\
   2 x_1 + 4 x_2 + 2 x_3+ x_4 + 3 x_5 &= -3 \\
 .\end{align*}

 Augmented matrix:
 \[
   \left[
   \begin{array}{ccccc;{2pt/2pt}c}
       1& 2  & 1  & 1  & 1  & 1\\
     -3  & -6  & -2  & 0 & -1 & -3\\
     2 & 4 & 2 & 1 & 3 & -3\\
   \end{array}
   \right]    \xrightarrow[ r_2 + 3 r_1 ]{ r_3 - 2 r_1} \left[
   \begin{array}{ccccc;{2pt/2pt}c}
       1 & 2  & 1 & 1 & 1 &1\\
       0& 0 & 1 &3 &2 &0\\
     0& 0& 0& -1&1 & -5\\
   \end{array}      \right]
 \]
  \[
    \xrightarrow[ r_3 \times -1]{}
    \left[
    \begin{array}{ccccc;{2pt/2pt}c}
        1 & 2  & 1 & 1 & 1 &1\\
        0& 0 & 1 &3 &2 &0\\
      0& 0& 0& 1&-1 & 5\\
    \end{array}
    \right]    \xrightarrow[ r_2 -3 r_3]{ r_1 - r_3}
    \left[
    \begin{array}{ccccc;{2pt/2pt}c}
        1 & 2  & 1 & 0 &2 &-4\\
        0& 0 & 1 &0 &5 &-15\\
      0& 0& 0& 1&-1 & 5\\
    \end{array}
    \right] 
  \]
  \[
    \xrightarrow[ ]{ r_1 - r_2}
    \left[
    \begin{array}{ccccc;{2pt/2pt}c}
        1 & 2  & 0  & 0  & -3  & 11\\
      0  & 0  & 1  & 0 & 5 & -15\\
      0 & 0 & 0 & 1 & -1 & 5\\
    \end{array}
    \right]
  .\] 
  \begin{align*}
    x_1 + 2 x_2 - 3 x_5 &= 11 \\
    x_3 + 5 x_5 &= -15 \\
    x_4 - x_5 &= 5\\
    x_2,x_5 &= \text{ (free variable)}\\
  .\end{align*}
  \\
  We assign aribitrary parameters to the free variables and express the pivot variables in terms of the free variables.
  \begin{align*}
    x_1 &= 11 - 2 t + 3 s \\
    x_2 &= t \\
    x_3 &= -15 - 5 s \\
    x_4 &= 5 + s \\
    x_5 &= s \\
    t, s &\in \mathbb{R} 
  \end{align*}
 }
 
 \nt{
 We can always express the pivot variables in terms of the free variables since in reduced row echelon form, all entries above and below a pivot entry are zero, thus ensuring that each equation only contains one pivot variable.
 }
 \dfn{Consistent/Inconsistent Systems :}{
    A linear system is said to be \underline{inconsistent} if it has no solutions. Otherwise, it is said to be \underline{consistent}.\\
 }
          \thm{ Existence and Uniqueness Theorem :}
          {
            A linear system is consistent if and only if the rightmost column of the augmented matrix is not a pivot column - i.e. if and only if an echolon form of the augmented matrix has no row of the form  $
[
  \underbrace{0 \; \cdots \; 0}_{i}
  \;|\;
  b
]      $ where $ b \neq 0$.\\
            If a linear system is consistent, then the solution set contains either 
            \begin{enumerate}[label=(\roman*)]
              \item A unique solution, (when ther are no free variables) or
              \item An infinite number of solutions, (when there are at least one free variable).
              \end{enumerate}
              i.e.  if a linear system is either has zero, one or an inifinite number of solutions.\\
          }
\nt{
Once we have reduced a matrix to echelon form, we can determine the number of solutions i.e. we can answer the existence and uniqueness questions. To find an explicit solution, we can further reduce the matrix to reduced row echelon form.\\
}
But suppose you're asked to sovle the following two systems:\\
 
\[
  \color{red}{
\left[
\begin{array}{cccc|c}
  a_{11} & a_{12} & \cdots & a_{1n} & b_{1} \\[2pt]
  a_{21} & a_{22} & \cdots & a_{2n} & b_{2} \\[4pt]
  \vdots & \vdots & \ddots & \vdots & \vdots \\[4pt]
  a_{m1} & a_{m2} & \cdots & a_{mn} & b_{m}
\end{array}}
\right]  \qquad  \color{black}{\text{ and }} \qquad  \color{blue}{ \left[
\begin{array}{cccc|c}
  a_{11} & a_{12} & \cdots & a_{1n} & c_{1} \\[2pt]
  a_{21} & a_{22} & \cdots & a_{2n} & c_{2} \\[4pt]
  \vdots & \vdots & \ddots & \vdots & \vdots \\[4pt]
  a_{m1} & a_{m2} & \cdots & a_{mn} & c_{m}
\end{array}
\right]  }
.\] 
To solve the \color{red}{first system}\color{black}{, we row reduce the augmented matrix to echelon form and solve but then to solve the }\color{blue}{second system}\color{black}{, we need to start the process all over again with a new right hand column, which seems like too much work! There should be a more efficient way. And there is, we just need to rethink the linear system in terms of matrix equations and learn some more properties of matrices.}\\
\section{ Matrix Operations: Addition}
 Suppose $ A$ and $ B$ are $ m \times  n$         matrices with entries $ a_{ i j}$ and $ b _{ i j }$ respectively, $ 1 \leq i \leq m$, $ 1 \leq j \leq n$. 
 \[
   a_{ij} \text{ is the entry in the $ i$-th row and $ j$-th column}
 .\] 
 The matrix $ A+B$ is defined to be the $ m \times  n$ matrix whose $ (i,j) $ entry is $ a_{ij}+ b _{ij}$ i.e. \[
 A+B =  \begin{bmatrix}
     a_{11} & a_{12} & a_{ 1 3} & \dots  & a_{1n} \\
     a_{21} & a_{ 2 2} & a_{ 2 3} & \dots  & a_{2n} \\
     \vdots & \vdots & \vdots & \ddots & \vdots \\
     a_{ m 1} & a_{m 2} & a_{ m 3} & \dots  & a_{mn}\end{bmatrix}  +   \begin{bmatrix}
     b_{11} & b_{12} & b_{ 1 3} & \dots  & b_{1n} \\
     b_{21} & b_{ 2 2} & b_{ 2 3} & \dots  & b_{2n} \\
     \vdots & \vdots & \vdots & \ddots & \vdots \\
     b_{ m 1} & b_{m 2} & b_{ m 3} & \dots  & b_{mn}\end{bmatrix}  
 .\]   is defined to be 
 \[
  A+B = \begin{bmatrix}
      a_{11} + b_{11} & a_{12} + b_{12} & a_{ 1 3} + b_{ 1 3} & \dots  & a_{1n} + b_{1n} \\
      a_{21} + b_{21} & a_{ 2 2} + b_{ 2 2} & a_{ 2 3} + b_{ 2 3} & \dots  & a_{2n} + b_{2n} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      a_{ m 1} + b_{ m 1} & a_{m 2} + b_{m 2} & a_{ m 3} + b_{ m 3} & \dots  & a_{mn}+b _{mn}\end{bmatrix}
 .\]  i.e.  addition is defined component-wise between two matrices of the same size.\\
 \nt{
 A vector $ \vec{ v} \in \mathbb{R} ^{n}$ can also be expressed as a column ( i.e. a matrix with one column) as follows:
 \[
 \text{ If }   \vec{ v} = \langle v_1, v_2, \ldots , v_n \rangle = \begin{bmatrix}
     v_1 \\[2pt]
     v_2 \\[2pt]
     \vdots \\[2pt]
     v_n
   \end{bmatrix}
 .\]
 In which case, we would want to know that vector addition (component-wise) coincides with matrix addition, and it clearly does.\\
 }

 \section{Matrix Operations: Scalar Multiplication}
 Suppose $ A$ is an $ m \times n$ matrix with entries $ a_{ i j}$, and let $  c \in \mathbb{R}$.\\
 The matrix $ cA$ is defined to be the $ m \times n$ matrix whose $ (i,j)$ entry is $ c a_{ i j}$ i.e.
 \[
  cA =  \begin{bmatrix}
      c a_{11} & c a_{12} & c a_{ 1 3} & \dots  & c a_{1n} \\
      c a_{21} & c a_{ 2 2} & c a_{ 2 3} & \dots  & c a_{2n} \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      c a_{ m 1} & c a_{m 2} & c a_{ m 3} & \dots  & c a_{mn}\end{bmatrix}
 .\] 
 Once again, this definition coincides with scalar multiplication of a vector.\\
 \\
 \section{Matrix Operations: Matrix Multiplication}
 Suppose $ A$ is an $ m \times  n$ matrix and $ B$ is a $ p \times  q$ matrix.\\ 
 The matrix product $ AB$ is only defined if $ n = p$ i.e. the number of columns in $ A$ is equal to the number of rows in $ B$.\\
 The resulting matrix $ AB$ is an $ m \times q$ matrix whose $ (i,j)$ entry is given by\[
 a_{i 1} b_{ 1 j}+ a_{i 2} b_{ 2 j} + \cdots + a_{i n} b_{ n j} 
 .\] 
 Note this equals the dot product of the $ i$-th row of $ A$ and the $ j$-th column of $ B$ i.e. 
 \[
  \begin{bmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\[2pt]
      a_{21} & a_{22} & \cdots & a_{2n} \\[2pt]
      \vdots & \vdots & \ddots & \vdots \\[2pt]
      a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
    \begin{bmatrix}
      b_{11} & b_{12} & \cdots & b_{1q} \\[2pt]
      b_{21} & b_{22} & \cdots & b_{2q} \\[2pt]
      \vdots & \vdots & \ddots & \vdots \\[2pt]
      b_{p1} & b_{p2} & \cdots & b_{pq}
      \end{bmatrix}
      = 
        \begin{bmatrix}
             c_{11} & c_{12} & c_{ 1 3} & \dots  & c_{1n} \\
             c_{21} & c_{ 2 2} & c_{ 2 3} & \dots  & c_{2n} \\
             \vdots & \vdots & \vdots & \ddots & \vdots \\
             c_{ m 1} & c_{m 2} & c_{ m 3} & \dots  & c_{mn}
        \end{bmatrix} 
 .\]
 where 
 \begin{align*}
   c_{ 1 1} &= a_{11} b_{11} + a_{12} b_{21} + \cdots + a_{1n} b_{n1} \\
    c_{ 1 2} &= a_{11} b_{12} + a_{12} b_{22} + \cdots + a_{1n} b_{n2} \\
    c_{ m j} &= a_{m1} b_{1j} + a_{m2} b_{2j} + \cdots + a_{mn} b_{nj} \\
 .\end{align*}
etc\\                   

 \nt{
 In order for the dot product of a row in $ A$ with a column in $ B$ to be defined, the number of columns in $ A$ must equal the number of rows in $ B$.\\
}
\ex{}{
 \[
 A = \begin{bmatrix}
 3 & 5\\
 -1 & -2\\
 \end{bmatrix} \qquad  B = \begin{bmatrix}
 4 & -2\\
 1 & 3\\
 \end{bmatrix}
 .\] 
 \[
 AB = \begin{bmatrix}
 3 \left( 4 \right) + 5 \left( 1 \right)  & 3 \left( -2 \right) + 5 \left(  3 \right) \\
  -1 \left( 4 \right) - 2 \left( 1 \right) & -1 \left( -2 \right) -2 \left( 3 \right)  \\
 \end{bmatrix} = \begin{bmatrix}
 17 & 9\\
 -6 & -4\\
 \end{bmatrix}
 .\] 
 \[
 \text{Note: } BA = \begin{bmatrix}
 4 & -2\\
 1 & 3\\
 \end{bmatrix} \begin{bmatrix}
 3 & 5\\
 -1 & -2\\
 \end{bmatrix} = \begin{bmatrix}
 14 & 24\\
 0 & -1\\
 \end{bmatrix}
 .\] 
 \textbf{ Beware!} $ AB \neq BA$ \\
 In general, matrix multiplication is not commutative.\\
 Sometimes $ BA$ isn't even defined, when $ AB $ is defined.\\
}
\ex{}{
Suppose $ A = \begin{bmatrix}
1 & -7 & 4  \\
\end{bmatrix}
$, $ B = \begin{bmatrix}
4 & 5 \\
1 & 0 \\
-5 & -1 \\
\end{bmatrix}$ 
\\
\[
AB = \begin{bmatrix}
1 & -7 & 4  \\
\end{bmatrix}
 \begin{bmatrix}
 4 & 5 \\
 1 & 0 \\
 -5 & -1 \\
 \end{bmatrix} = \begin{bmatrix}
 -23 & 1\\
 \end{bmatrix}
.\]
while $ BA$ isn't defined since the number of columns in $ B = 2 \neq $ the number of rows in $ A = 1$.\\
}
With our new matrix arithmetic we can rewrite our $ m \times  n$ linear system as a matrix equation.\\
\begin{align*}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n &= b_1 \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n &= b_2 \\
  \vdots & \vdots \\
  a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n &= b_m
.\end{align*}
can be rewritten as:
\[
  \underbrace{
\begin{bmatrix}
  a_{11} & a_{12} & \cdots & a_{1n} \\[2pt]
  a_{21} & a_{22} & \cdots & a_{2n} \\[4pt]
  \vdots & \vdots & \ddots & \vdots \\[4pt]
  a_{m1} & a_{m2} & \cdots & a_{mn} 
\end{bmatrix} 
  }_{ m \times  n }
  \underbrace{  
\begin{bmatrix}
  x_1 \\[2pt]
  x_2 \\[2pt]
  \vdots \\[2pt]
  x_n
\end{bmatrix} }_{  n \times  1}  = \underbrace{   \begin{bmatrix} 
  b_1 \\[2pt]
  b_2 \\[2pt]
  \vdots \\[2pt]
  b_m
\end{bmatrix} }_{ m \times  1}
.\] 
or $ A \vec{ x} = \vec{ b} $ where $ A$ is the $ m \times  n $ matrix 
\[
\vec{ x} = \begin{bmatrix}
  x_1 \\[2pt]
  x_2 \\[2pt]
  \vdots \\[2pt]
  x_n
\end{bmatrix} \in \mathbb{R} ^{n} , \qquad  \vec{ b} = \begin{bmatrix}
  b_1 \\[2pt]
  b_2 \\[2pt]
  \vdots \\[2pt]
  b_m
\end{bmatrix} \in \mathbb{R} ^{m}
.\] 
\dfn{Coefficient Matrix :}{
   $ A$  above is known as the \underline{coefficient matrix} of the linear system. It contains the coefficients from the system  but not the constants from the right hand side of the equations.\\
}
\textbf{Note:} the difference between the coefficient matrix and the augmented matrix.\\
\\
In the future, we will formulate problems involving linear systems in terms of matrix equations and use matrix arithmetic to solve them, but first we'll consider some futer properties of matrix multiplication.\\
\section{Matrix Operations: Properties of Matrix Multiplication}
Suppose $ A$ and $ B$ are $ m \times  n$ matrices with $ (i,j) $ entries given by $ a_{ij}$ and $ b_{ij}$ respectively.\\
Suppose $ \vec{ u} , \vec{ w} \in \mathbb{R} ^{m}$ with $ \vec{ u} = \begin{pmatrix} u_1\\ \vdots\\ u_m \end{pmatrix} $, $ \vec{ v} = \begin{pmatrix} v_1\\ \vdots\\ v_m \end{pmatrix}$ and  suppose $ c \in \mathbb{R}$ \\
The following are true:
\begin{enumerate}[label=(\roman*)]
  \item $ A \left( B+C \right) = AB +AC $  (left distributive law)
  \item  $ \left( A + B \right) \left( C \right) = AC +BC$ (right distributive law)
  \item  $ \left( c A \right) \left( B \right) = C \left( AB \right) = A \left( cB \right) $
  \item $ \left( AB \right) C = A \left( BC \right) $ (associative law) 
  \end{enumerate}
  We will prove these identities by multiplying both sides of the equations by a "test vector" $ \vec{ x} $ and checking that both sides agree.\\
  Since every $ \vec{ x} \in \mathbb{R} ^{n}$ can be written as a sum of the standard basis vectors, scaled by some constants
  \[
  \vec{ x} = \begin{pmatrix} x_1\\ \vdots\\ x_n \end{pmatrix}= x_1 \begin{pmatrix} 1\\ 0\\ \vdots\\ 0 \end{pmatrix} + x_2 \begin{pmatrix} 0\\ 1\\ \vdots\\ 0 \end{pmatrix} + \cdots + x_n \begin{pmatrix} 0\\ 0\\ \vdots\\ 1 \end{pmatrix} = x_1 \vec{ e_1} + x_2 \vec{ e_2} + \cdots + x_n \vec{ e_n}
  .\] 
  it is sufficient to prove the properties when the test vector $ \vec{ x} $ is $ \vec{ e_j} $.\\
  We will make use of the following properties (proven above):\\
          \begin{enumerate}[label=(\roman*)]
            \item $ A \left( \vec{ u} + \vec{ w}  \right) = A \vec{ u} + A \vec{ w} $
            \item     $ \left( A+B \right) \vec{ u} = A \vec{ u} + B \vec{ u} $
            \item $ c \left( A \vec{ u}  \right) = \left( c A \right) \vec{ u} = A \left( c \vec{ u}  \right) $
            \end{enumerate}  \\
            \\
            \\
            
            
  \begin{enumerate}[label=(\roman*)]
    \item 
      \begin{align*}
        \left( A \left( B+C \right)  \right) \vec{ e_j} &= j ^{ \text{th}} \text{ column of } A \left( B+C \right) \\
        &= A \left(  \left( B+C \right)  \vec{ e_j}  \right) \ldots \text{ using the fact that } \underbrace{ \left( BA \right) \vec{ e_j} =B \left( A \vec{ e_j}  \right)  }_{ \star }\\
        &= A \left( B \vec{ e_j} + C \vec{ e_j}  \right) \ldots \text{ using (i) above}\\
        &= A \left( B \vec{ e_j}  \right) + A \left( C \vec{ e_j}  \right) \ldots \text{ using (i) above}\\
        &= \left( AB \right) \vec{ e_j} + \left( AC \right) \vec{ e_j} \ldots \text{ by } \star\\
        &= \left( AB + AC \right) \vec{ e_j} \ldots \text{ by (ii) above}\\
        \implies A \left( B+C \right)  &= \left( AB + AC \right) 
      .\end{align*}
    \item  
      \begin{align*}
        \left( \left( A+B \right) C \right) \vec{ e_j} &= \left( A+B \right) \left( C \vec{ e_j}  \right) \text{ by } \star\\
        &= A \left( C \vec{ e_j}  \right) + B \left( C \vec{ e_j}  \right) \text{ by (ii)} \\
        &= \left( AC \right) \vec{ e_j} + \left( BC \right) \vec{ e_j} \text{ by } \star\\
        &= \left( AC + BC \right) \vec{ e_j} \text{ by (ii)}\\
        \implies \left( A+B \right) C &= AC + BC
      .\end{align*}
    \end{enumerate}
    We've just seen some properties that marix multiplication satisfies. It's also important to note that matrix multiplication does not satisfy.
    \nt{
    \begin{enumerate}[label=(\arabic*).]  
      \item In general, $ AB \neq BA$ i.e. matrix multiplication is not commutative.
      \item    The cancellation law does not hold for matrix multiplication in general,\\
        i.e. if $ AB = AC$ then it is not true in general that $ B = C$.
        \raggedcolumns
        \begin{multicols}{2}
        \[
        \text{ Example: } \overset{A}{ \begin{bmatrix}
        1 & 0\\
        0 & 0\\
        \end{bmatrix}} \overset{B}{ \begin{bmatrix}
        a & b\\
        x & y\\
    \end{bmatrix}} = \overset{A}{ \begin{bmatrix}
    1 & 0\\
    0 & 0\\
\end{bmatrix}} \overset{C}{ \begin{bmatrix}
a & b\\
w & z\\
\end{bmatrix}}
        .\] 
        \[
        \begin{bmatrix}
        a & b\\
        0 & 0\\
        \end{bmatrix}= \begin{bmatrix}
        a & b\\
        0 & 0\\
        \end{bmatrix}
        .\] 
        but if $ x \neq w$ and $ y \neq z$ then $ B \neq C$.\\
        \break
              Numerical example:\\
              \[
              \overset{A}{ \begin{bmatrix}
              1 & 3\\
              2& 4\\
          \end{bmatrix} } \overset{B}{ \begin{bmatrix}
          1 & 2\\
          0 & 0\\
      \end{bmatrix}} = \overset{AB}{ \begin{bmatrix}
      1 & 2\\
      2 & 4\\
  \end{bmatrix}}
              .\] 
              \[
              \underbrace{ \begin{bmatrix}
              1 & 5\\
              2 & 6\\
              \end{bmatrix} }_{ A } \underbrace{ \begin{bmatrix}
              1 & 2\\
              0 & 0\\
              \end{bmatrix} }_{ C } = \underbrace{ \begin{bmatrix}
              1 & 2\\
              2 & 4\\
        \end{bmatrix} }_{ AC }
              .\] 
        \end{multicols}
      \item If $ AB$ is the zero matrix then it is not necessarily true that $ A$ or $ B$ is the zero matrix.\\
        \[
        \text{ Example: } \begin{bmatrix}
        0 & 5\\
        0 & 6\\
        \end{bmatrix} \begin{bmatrix}
        1 & 1\\
        0 & 0\\
        \end{bmatrix} = \begin{bmatrix}
        0 & 0\\
        0 & 0\\
        \end{bmatrix}
        .\] 
    \end{enumerate}
    }
      
    
    
    
  
  



 

 
   
   
 

          
 
 
 
 
 
 

   

\end{document}     
