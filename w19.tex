\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}

%\usepackage[tagged, highstructure]{accessibility}
\usepackage{tocloft}
\usepackage{arydshln}
\usetikzlibrary{arrows.meta, decorations.pathreplacing}
\usepackage{tikz-cd}
\usepackage{polynom}
\usepackage{pifont}
\newcommand{\pistar}{{\zf\symbol{"4A}}}
% a tiny helper for a stretched phantom (for the underbrace)
\newcommand\mc[1]{\multicolumn{1}{c}{#1}}



\begin{document}
\title{Linear Algebra I}
\author{Lecture Notes Provided by Dr.~Miriam Logan.}
\date{}
\maketitle
\tableofcontents
\newpage  
         \ex{}{
          Let $ \mathcal{P} _1 \left[ x \right] $ be the set of polynomials of degree at most 1.\\
          Let $ \langle f,g  \rangle = \int_{0}^{1} f\left( x \right) g \left( x \right) dx $ \\
          $ \langle ,  \rangle $ is an inner product defined on $ \mathcal{P} _1 \left[ x \right] \times  \mathcal{P} _1 \left[ x \right] $ (already prvoen)\\
          \\
          $ \mathcal{B} = \left\{ 1 , x \right\} $ is not orthogonal :\\
          \[
          \langle 1,x  \rangle = \int_{0}^{1} x dx = \frac{ x^2  }{ 2 } \Big|_{0}^{1} = \frac{1}{2}
          .\] 
          Use the Gram-Schmidt orthogonalization process  to orthogonalize $  \mathcal{B}$.\\
          Let $ \vec{ w_1} =1$
          \begin{align*}
           \vec{ w_2} &= x - \frac{ \langle 1,x \rangle }{ \langle 1,1 \rangle } 1\\
           &= x - \frac{ \int_{0}^{1} x dx   }{ \int_{0}^{1} 1 dx } 1  \\
           &= x - \frac{ \frac{1}{2} }{ 1 } 1 \\
           &= x - \frac{1}{2}
          .\end{align*}   
          $ \left\{ 1, x - \frac{1}{2} \right\}$  is an orthogonal basis for $ \mathcal{P} _1 \left[ x \right] $.\\
          To get an orthonormal basis
          \begin{align*}
           \| 1\| &= \sqrt{ \langle 1,1 \rangle } = \sqrt{ \int_{0}^{1} 1 dx } = \sqrt{ 1 } = 1\\
           \|x - \frac{1}{2}\|  &= \sqrt{ \langle x - \frac{1}{2}, x - \frac{1}{2} \rangle } = \sqrt{ \int_{0}^{1} \left( x - \frac{1}{2} \right)^2 dx }\\
           &= \sqrt{ \int_{0}^{1} \left( x^2 - x + \frac{1}{4} \right) dx }= \sqrt{ \frac{ x^3  }{ 3 } - \frac{ x^2  }{ 2 } + \frac{ x  }{ 4 } \Big|_{0}^{1} }\\
           &= \sqrt{ \frac{1}{3} - \frac{1}{2} + \frac{1}{4} } = \sqrt{ \frac{4 - 6 + 3}{12} } = \sqrt{ \frac{1}{12} } 
          .\end{align*}
          \[
          \frac{  \vec{ w_2}   }{ \| \vec{ w_2} \| } = \frac{ x - \frac{1}{2} }{ \sqrt{ \frac{1}{12} } } = \sqrt{ 12 } \left( x - \frac{1}{2} \right) 
          .\] 
          \[
          \text{ orthonormal basis } = \left\{ 1, \sqrt{ 12 } \left( x - \frac{1}{2} \right) \right\}
          .\] 
         }
         \dfn{ :}{
         Suppose $ \langle ,  \rangle : V \times  V \to \mathbb{R}$ is a symmetric bilinear form defined on a real vector space $ V$. \\
         Two vectors $ \vec{ u}, \vec{ v} \in V$ are called orthogonal if $ \langle \vec{ u}, \vec{ v} \rangle = 0$.\\
         }
         \section{Orthogonal basis for symmetric bilinear forms}
         \thm{}
         {
         Suppose $ \langle ,  \rangle $ is a bilinear symmetric form defined on a vector space $ V$. There exists an orthogonal basis 
         \[
         \left\{ \vec{ w_1}, \ldots, \vec{ w_n} \right\} \text{ of } V \qquad  \left(  \text{ i.e. } \langle \vec{ v_i} , \vec{ v_j}\rangle =0 \qquad  \forall  i \neq j  \right) 
         .\] 
         }

         \pf{Proof:}{
         (by induction)\\
         $ n=1$. If $ V$ is one dimensional the statement is true.\\
         \\
         Assume for an $ \left( n-1 \right) $ dimensional vector space $ V$ there exists an orthogonal basis.\\
         \\
         \underline{Case 1:}\\
         $ \langle \vec{ u} ,\vec{ v}   \rangle =0$ $ \forall  \vec{ u} , \vec{ v} \in V$ in which case all bases are orthogonal.\\
         \\
         \underline{Case 2:}\\
         Suppose there exists $ \vec{ u} , \vec{ v} \in V$  such that $ \langle \vec{ u} ,\vec{ v}   \rangle \neq 0$.\\
         Then
         \[
         \langle \vec{ u} + \vec{ v} , \vec{ u} + \vec{ v}   \rangle = \langle \vec{ u} , \vec{ u}   \rangle + 2 \langle \vec{ u} , \vec{ v}  \rangle + \langle \vec{ v} , \vec{ v}   \rangle
         .\] 
         and so one out of $  \langle \vec{ u} + \vec{ v} , \vec{ u} + \vec{ v}   \rangle $, $ \langle \vec{ u}  , \vec{ u}   \rangle $ and  $ \langle \vec{ v} , \vec{ v}   \rangle $ are non-zero.\\
         \[
         \implies \exists  \quad \vec{ v_1} \in V \text{ such that } \langle \vec{ v_1} , \vec{ v_1}   \rangle \neq 0
         .\] 
         Let $ u = span  { \vec{ v_1} }$ and 
         \[
          W = U ^{\perp} = \left\{ \vec{ w} \in V \mid \langle \vec{ w} , \vec{ v_1}   \rangle =0 \right\}
         .\] 
         \underline{Claim:} $ V = U \bigoplus_{} W$ \\
         If this is the case then $ \left\{ \vec{ v_1} , \vec{ v_2} ,\ldots , \vec{ v_n}  \right\} $  is orthogonal basis for $ V$ where $ \left\{ \vec{ v_2} ,\ldots , \vec{ v_n}  \right\} $ is an orthogonal basis for $ W$.\\
         To check $ V = U \bigoplus_{} W$ \\
         Let $ \vec{ v} \in V$, $ \vec{ v} = \underbrace{ \frac{ \langle \vec{ v_1} ,\vec{ v_1}   \rangle   }{ \langle \vec{ v_1} ,\vec{ v_1}   \rangle  } \vec{ v_1}  }_{  \in U} +  \underbrace{ \vec{ v} - \frac{ \langle \vec{ v} , \vec{ v_1}   \rangle   }{ \langle \vec{ v_1} , \vec{ v_1} \rangle  } \vec{ v_1}  }_{ \in U ^{\perp} = W }$\\
         since 
         \begin{align*}
          \langle \vec{ v_1} ,\vec{ v} - \frac{ \langle \vec{ v} , \vec{ v_1}   \rangle   }{ \langle \vec{ v_1} , \vec{ v_1}   \rangle   } \vec{ v_1}   \rangle &= \langle \vec{ v_1} ,\vec{ v}   \rangle - \frac{ \langle \vec{ v} , \vec{ v_1}   \rangle   }{ \langle \vec{ v_1} , \vec{ v_1}   \rangle   }  \langle \vec{ v_1} ,\vec{ v_1}   \rangle  \\
          &= \langle \vec{ v_1} ,\vec{ v}   \rangle - \langle \vec{ v} , \vec{ v_1}   \rangle =0
         .\end{align*}
         Linear cominations are unique $  \iff U \cap W = \{ \vec{ 0} \}$ \\
         Suppose $ \vec{ v} \in U \cap  W$
         \[
         \implies \vec{ v} \in U, \vec{ v} = c \vec{ v_1} \text{ where } c \in \mathbb{R}
         .\] 
         \[
         \vec{ v} \in W \implies \langle \vec{ v} , \vec{ v_1}   \rangle =0
         .\] 
         \[
         \langle c \vec{ v_1} , \vec{ v_1}   \rangle =0 \implies c \langle \vec{ v_1} , \vec{ v_1}   \rangle =0
         .\] 
         \[
         c=0 \qquad  \text{ or } \qquad  \langle \vec{ v_1} , \vec{ v_1}   \rangle =0
         .\] 
         \[
         \text{ Recall } \langle \vec{ v_1} , \vec{ v_1}   \rangle \neq 0 \qquad  \implies c=0 \text{ and } \vec{ v} = 0 \left( \vec{ v_1}  \right) = \vec{ 0} 
         .\] 
         \[
         \text{ i.e.  } U \cap W = \{ \vec{ 0} \} \qquad  \implies V = U \bigoplus_{} W
         .\] 
         
         }
         \thm{}
         {
         Suppose $ A$ is a real $n \times n$  symmetric matrix. Then there exists an invertible matrix $ B$ such that $ B ^{T}A B$ is a diagonal matrix.\\
         }
         \pf{Proof:}{
         If $ A$ is real and symmetric it defineds a symmetric bilinear form on $ \mathbb{R} ^{n}$.\\
         \[
         \langle \vec{ x} , \vec{ y}   \rangle = \left( \vec{ x}  \right) ^{ T} A \vec{ y} \qquad  \forall  \vec{ x} , \vec{ y} \in \mathbb{R} ^{n}
         .\] 
         We know there exists an orthogonal basis for this symmetric bilinear form $  \mathcal{B}= \left\{ \vec{ v_1} ,\ldots , \vec{ v_n}  \right\}  $ with $ \langle \vec{ v_i} , \vec{ v_j}   \rangle =0 \qquad  \forall  i \neq j$.

          \[
 \text{ Let } B
  = \begin{bmatrix}
      \vline           & \vline           & \dots  & \vline \\
      \vec v_{1}  & \vec v_{2}  & \dots  & \vec v_{n} \\[4pt]
      \vline           & \vline           & \dots  & \vline
    \end{bmatrix}
\]

        then the matrix of the bilinear form with respect to $ \mathcal{B}$ is $ B ^{T}A B$\\
        Note: $ \left( B ^{T}A B \right) _{ i j}= \langle \vec{ v_i} , \vec{ v_j}   \rangle  =0 $ if $ i \neq j$ \\
        $ \implies B ^{T}A B$ is a diagonal matrix.\\
         }

 \textit{ We will prove next week that for a symmetric matrix $ A$ with distinct eigenvalues, the eigenvectors of $ A$ are orthogonal.}\\
         
 \\
 \ex{}{
 Let $ A = \begin{bmatrix}
 6 & 2\\
 2 & 3\\
 \end{bmatrix}$. Find a baisis $ \mathcal{B}$ of $ \mathbb{R} $ of $ \mathbb{R} ^2$ such that $ B^{T}A B$ is diagonal.\\
 \begin{align*}
  \chi _A \left( \lambda \right) &= \left( 6-\lambda \right)  \left( 3 - \lambda \right) -4 =0 \\
  &= 18 - 9 \lambda + \lambda ^2 -4 =0 \\
  &= \lambda ^2 - 9 \lambda + 14 =0\\
  &= \left( \lambda - 7 \right) \left( \lambda - 2 \right) =0 \\
  & \implies \lambda _1 = 7, \lambda _2 = 2

 .\end{align*}
 \[
 \lambda=7: \qquad  \left[
 \begin{array}{cc;{2pt/2pt}c}  
   -1 & 2 & 0\\
   2 & -4 & 0\\
 \end{array}
\right] \qquad  x = 2y \qquad  \text{ Let } \vec{ v_1} =  \begin{bmatrix}
2\\
1\\
\end{bmatrix}
 .]
 \[
 \lambda=2: \qquad  \left[
 \begin{array}{cc;{2pt/2pt}c}  
   4 & 2 & 0\\
   2 & 1 & 0\\
 \end{array}
 \right] \qquad  -2x =y \qquad  \text{ Let } \vec{ v_2} =  \begin{bmatrix}
 1\\
 -2\\
 \end{bmatrix}
 .\] 
 Let $ \mathcal{F} = \left\{ \begin{bmatrix}
 2\\
 1\\
 \end{bmatrix}
 , \begin{bmatrix}
 1\\
 -2\\
 \end{bmatrix}
  \right\} $. Clearly $ \mathcal{F}$ is orthogonal .\\
  Let $ B = \begin{bmatrix}
  2 & 1\\
  1 & -2\\
  \end{bmatrix}$ \\

  \begin{align*}
   B ^{T} A B &= \begin{bmatrix}
   2 & 1\\
   1 & -2\\
   \end{bmatrix} \begin{bmatrix}
   6 & 2\\
   2 & 3\\
   \end{bmatrix} \begin{bmatrix}
   2 & 1\\
   1 & -2\\
   \end{bmatrix}\\
   &= \begin{bmatrix}
   14 & 7\\
   2 & -4\\
   \end{bmatrix} \begin{bmatrix}
   2 & 1\\
   1 & -2\\
   \end{bmatrix}= \begin{bmatrix}
   35 & 0\\
   0 & 10\\
   \end{bmatrix}
  .\end{align*}
  Recall: $ B^{-1} A B = \begin{bmatrix}
  7 & 0\\
  0 & 2\\
  \end{bmatrix}$.\\
  In general, if $ \langle ,  \rangle : \mathbb{R} ^{n} \times  \mathbb{R} ^{n} \to \mathbb{R}$ is a bilinear form defined by $ \langle \vec{ x} ,\vec{ y}   \rangle = \left( \vec{ x}  \right) ^{T} A \vec{ y} $ for some $ A \in M_{n\times n} \left( \mathbb{R} \right) $ and $ \mathcal{F} = \left\{ \vec{ v_1} , \ldots , \vec{ v_n}  \right\} $ is a basis consisting of eigenvectors of $ A$ , then
  \begin{align*}
   \langle \vec{ x} ,\vec{ y}   \rangle = \sum\limits_{1 \leq i , j \leq n}^{}  x_i y_j \langle \vec{ v_i} ,\vec{ v_j}   \rangle \\
   &= \sum\limits_{i , j } ^{}  x_i y_j \left( \vec{ v_i}  \right) ^{T} A \vec{ v_j} \\
   &= \sum\limits_{i , j } ^{}  x_i y_j \lambda _i \left( \vec{ v_i}  \right) ^{T} \vec{ v_j} \\
  .\end{align*}
  Therfore, in the example above, with 
  \[
  \vec{ v_1} = \begin{bmatrix}
   2\\
  1\\
  \end{bmatrix}
  , \qquad  \vec{ v_2} = \begin{bmatrix}
  1\\
  -2\\
  \end{bmatrix}
      \qquad  \lambda_1 = 7, \lambda_2 = 2
  .\] 
  \begin{align*}
   \left( B ^{T}A B \right) _{ 1 1} &= \langle \vec{ v_1} , \vec{ v_1}   \rangle = \lambda_1 \left( 2^2+1^2 \right)  = 7 \left( 5 \right) = 35\\
   \left( B ^{T}A B \right) _{ 1 2} &= \langle \vec{ v_1} , \vec{ v_2}   \rangle = 0 = \left( B^{T}A B \right) _{ 2 1}\\
   \left( B ^{T}A B \right) _{ 2 2} &= \langle \vec{ v_2} , \vec{ v_2}   \rangle = \lambda_2 \left( 1^2+(-2)^2 \right)  = 2 \left( 5 \right) = 10
  .\end{align*}
  Motivation for defining and equivalent for on $ \mathbb{C} ^{n} \times  \mathbb{C} ^{n}$.\\
 }
 
    Next we wish to define an equivalent inner product on $ \mathbb{C} ^{n} \times  \mathbb{C} ^{n}$. The inner product $ \langle \vec{ x}, \vec{ y}   \rangle = \left( \vec{ x}  \right) ^{T} \vec{ y} $  defined on $ \mathbb{R} ^{n}$ won't work on $ \mathbb{C} ^{n}$ since 
    \[
    \langle \vec{ x} , \vec{ x}   \rangle = \sum\limits_{i=1}^{n} x_i ^2 \qquad  \text{ could be negative.}
    
    .\] 
    A natural dot product to defind on $ \mathbb{C} ^{n}$ is $ \langle ,  \rangle : \mathbb{C} ^{n} \times  \mathbb{C} ^{n} \to \mathbb{C}$
    \[
    \langle \vec{ z} ,\vec{ w}   \rangle = \sum\limits_{i=1}^{n} \overline{ z_i} w_i = \overline{ \vec{ z}  } ^{T} \vec{ w} 
    .\] 
    \[
    \text{ since }  \langle \vec{ z} , \vec{ z}   \rangle = \sum\limits_{i=1}^{n} \overline{ z_i} z_i = \sum\limits_{i=1}^{n} |z_i|^2 \geq 0 \qquad  \forall  \vec{ z} \in \mathbb{C} ^{n}
    .\] 
    \dfn{ :}{
    A sesquilinear form defined on a complex vector space $ V$  is a map 
    \[
    f : V \times  V \to \mathbb{C}
    .\] 
    such that
    \begin{enumerate}[label=(\arabic*).]  
      \item $ f$ is conjugate linear in the first variable, i.e. $ \forall  \lambda \in \mathbb{C}$, $ \vec{ v_1} , \vec{ v_2} , \vec{ w} \in \mathbb{C} ^{n}$
       \[
       f \left( \lambda \vec{ v_1} + \vec{ v_2} , \vec{ w}  \right) = \overline{ \lambda} f \left( \vec{ v_1} , \vec{ w}  \right) + f \left( \vec{ v_2} , \vec{ w}  \right)
       .\] 
      \item $f$ is linear in the second variable,
       \[
       f \left( \vec{ w} , \lambda \vec{ v_1} + \vec{ v_2}  \right) = \lambda f \left( \vec{ w_1} , \vec{ v_1}  \right) + f \left( \vec{ w_1} , \vec{ v_2}  \right) 
       .\] 
    \end{enumerate}
    
    }
    \ex{}{
    The dot product on $ \mathbb{C} ^{n}$ defined as 
    \[
    \langle \vec{ x} , \vec{ y}   \rangle  = \overline{ \vec{ x}  } ^{T} \vec{ y} = \sum\limits_{i=1}^{n} \overline{ x_i} y_i
    .\] 
    is a sesquilinear:\\
    Let $ \lambda \in \mathbb{C}$, $ \vec{ x} , \vec{ y} , \vec{ u} \in \mathbb{C} ^{n}$ \\
 \begin{align*}
  \langle \lambda \vec{ x} + \vec{ u} , \vec{ y}   \rangle &= \overline{ \left( \lambda \vec{ x} + \vec{ u}  \right) } ^{T} \vec{ y} \\
  &= \overline{ \lambda \left( \vec{ x}  \right) ^{T} + \vec{ u} ^{T} } \vec{ y} \\
  &= \left( \lambda \overline{ \left( \vec{ x}  \right) ^{T}} + \overline{\left( \vec{ u}  \right) ^{T}} \right) \left( \vec{ y}  \right) \\
  &= \overline{ \lambda} \overline{ \left( \vec{ x}  \right) ^{T}} \vec{ y} + \overline{ \left( \vec{ u}  \right) ^{T}} \vec{ y} \\
  &= \overline{ \lambda} \langle \vec{ x} , \vec{ y}   \rangle + \langle \vec{ u} , \vec{ y}   \rangle\\
  \langle \vec{ y} , \lambda \vec{ x} + \vec{ u}   \rangle &= \overline{ \vec{ y}  } ^{T} \left( \lambda \vec{ x} + \vec{ u}  \right)\\
  &= \overline{ \vec{ y}  } ^{T} \left( \lambda \vec{ x}  \right) + \overline{ \vec{ y}  } ^{T} \vec{ u}\\
  &= \lambda \overline{ \vec{ y}  } ^{T} \vec{ x} + \overline{ \vec{ y}  } ^{T} \vec{ u}\\
  &= \lambda \langle \vec{ y} , \vec{ x}   \rangle + \langle \vec{ y} , \vec{ u}   \rangle
 .\end{align*}
    }
    In a similar mannar to the real case, a sesquilinear form can be defined using an $n \times n$  matrix with complex entries aftere fixing a basis $ \mathcal{B} = \left\{  \vec{ b_1} , \ldots , \vec{ b_n}  \right\} $  for $ \mathbb{C} ^{n}$.\\
    \[
    \text{ If } \vec{ x} , \vec{ y} \in \mathbb{C} ^{n}, \qquad  \vec{ x} = \sum\limits_{i=1}^{n} x_i \vec{ b_i} , \qquad  \vec{ y} = \sum\limits_{i=1}^{n} y_i \vec{ b_i}\qquad \forall x_i, y_i \in \mathbb{C}
    .\] 
    
    \begin{align*}
     f \left( \vec{ x} , \vec{ y}  \right) &= f \left( \sum\limits_{i=1}^{n} x_i \vec{ b_i} , \sum\limits_{j=1}^{n} y_j \vec{ b_j}  \right)\\
    &= \sum\limits_{1 \leq i , j \leq n}^{n} \overline{ x_i} y_j f \left( \vec{ b_i} , \vec{ b_j}  \right)= \overline{ \left( \vec{ x}  \right) ^{T} } A \left(\vec{ y}\right) 
    .\end{align*}
    where $ A$ is $n \times n$  matrix whose $ (i,j) $ entry is $ f \left( \vec{ b_i} , \vec{ b_j}  \right)$.\\
    \underline{Notation:} We can use $ \vec{ x} ^{*}$ to denote $ \overline{ \left( \vec{ x}  \right) ^{T}}$ \\
    \mlem{}{
    Suppose $ A$ and $ B$ are $ m \times  k$ and $ k \times  n$ matrices respectively. Then
    \[
     \overline{ AB} = \overline{ A} \overline{ B}
    .\] 
   }
   
   \pf{Proof:}{
      \begin{align*}
       \left( \overline{AB} \right)_{ i j} = \overline{  \left( A B \right) _{ i j}} &= \overline{ \left( \sum\limits_{\ell =1}^{k} a_{ i \ell} b_{ \ell j} \right) } = \sum\limits_{\ell =1}^{k} \overline{ a_{ i \ell} b_{ \ell j}}\\
       &= \sum\limits_{\ell =1}^{k} \overline{ a_{ i \ell}} \overline{ b_{ \ell j}}\\
       &=  \left( \overline{A}  \overline{B}\right) _{ i j}\\
       &\implies \overline{AB} = \overline{A} \overline{B}
      .\end{align*}
   }
   \nt{
   The equivalent of a symmetric bilinear form for complex vector spaces is a Hermitian form.
   }
   Suppose $ V$ is a complex vector space.
   \dfn{Hermitian Form :}{
   A \underline{Hermitian form} is a squesilinear form $ f: V \times  V \to \mathbb{C}$ that satisfies
   \[
   f \left( \vec{ u} ,\vec{ v}  \right) = \overline{ f \left( \vec{ v} ,\vec{ u}  \right)} \qquad  \text{( conjugate symmetry)}
   .\] 
   }
   \thm{}
   {
   Suppose $ f : V \times  V \to \mathbb{C}$ is a sesquilinear form defined on a complex vector space $ V$.  Suppose $ A$ is the matrix of $ f$ with respect to the basis $ \mathcal{B} = \left\{  \vec{ b_1} , \ldots , \vec{ b_n}  \right\} $. Then $ f$ is a Hermitian form if and only if $ A ^{*} = A$  i.e.  $ \overline{A ^{T}} = A$
   }
   \pf{Proof:}{
    \begin{align*}
     f \left( \vec{ u} ,\vec{ v}  \right) &= \overline{f \left( \vec{ v} , \vec{ u}  \right) }\\
     \iff  \vec{ u} ^{*} A \vec{ v} &= \overline{ \vec{ v} ^{*} A \vec{ u} }\\
     \text{ i.e. } \vec{ u} ^{*} A \vec{ v} &= \overline{ \overline{\left( \vec{ v} ^{T} \right) }} \overline{A} \overline{ \vec{ u}}\\
     \vec{ u} ^{*} A \vec{ v} &= \vec{ v} ^{T} \overline{A} \overline{ \vec{ u}}    \\
     \sum\limits_{1 \leq i , j \leq n}^{} \overline{ u_i} v_j \left( A \right) _{ i j} &=  \sum\limits_{1 \leq i , j \leq n}^{} v_i \overline{ u_j} \left( \overline{A} \right) _{ i j}\\
     \text{ which happens } \iff A_{ij} &= \overline{A_{ji}} \\
     \text{ i.e. } A &= \overline{A ^{T}} \\
     A &= A ^{*} 
    .\end{align*}
   }
   \dfn{Hermitian Matrices :}{
   $ A \in M _{ n \times n} \left( \mathbb{C} \right) $ is said to be \underline{Hermitian} if $ A ^{*} = A$ 
   }
   \nt{
   A matrix with real entries, $ A \in M _{ n \times  n } \left( \mathbb{R} \right) $ also defines a sesquilinear form on $ \mathbb{C} ^{n}  \times  \mathbb{C} ^{n} $ 
   \[
   \langle \vec{ x} ,\vec{ y}   \rangle = \overline{ \vec{ x}  } ^{T} A \vec{ y} 
   .\] 
   The form is Hermitian if  $ \overline{A ^{T}}= A$ i.e.  $ A ^{T}= A$ \\
   $ \implies$ a real symmetric matrix defines a bilinear form on $ \mathbb{C} ^{n}  \times  \mathbb{R} ^{n}$ and a Hermitian sesquilinear form on $ \mathbb{C} ^{n}  \times  \mathbb{C} ^{n}$.
   }
   \dfn{ :}{
   An \underline{inner product} $ f : V \times  V \to \mathbb{C}$ on a complex vector space $ V$ is a \underline{Hermitian form} that is positive definite, i.e. $ f \left( \vec{ v} , \vec{ v}  \right) \ge 0 \qquad  \forall  \vec{ v} \in V$ with equality if and only if $ \vec{ v} = \vec{ 0}$.\\
   }
   \ex{}{
     Dot product on $ \mathbb{C} ^{n}$
     \[
     \langle \vec{ x} , \vec{ y}   \rangle = \sum\limits_{i=1}^{n} \overline{x_i} y_i 
     .\] 
     \begin{itemize}
      \item Already show to be a sesquilinear form.
      \item Hermitian?\\
       \[
        \langle \vec{ x} ,\vec{ y}   \rangle = \sum\limits_{i=1}^{n} \overline{x_i} \overline{  \overline{y_i}} = \sum\limits_{i=1}^{n}  \overline{ \overline{y_i} x_i} = \overline{ \sum\limits_{i=1}^{n} \overline{y_i} x_i} = \overline{ \langle \vec{ y} ,\vec{ x}   \rangle }
       .\] 
      \item Positive definite?\\
       \begin{align*}
        \langle \vec{ x} ,\vec{ x}   \rangle &= \sum\limits_{i=1}^{n} | x_i | ^2 \ge 0 \text{ with equality}\\
        \iff x_i &= 0 \qquad  \forall  i \\
       \text{ i.e. } \vec{ x} &= \vec{ 0}
        \implies \text{ The dot product on } \mathbb{C} ^{n} \text{ is an inner product.}
       .\end{align*}
     \end{itemize}
     The matrix of the dot product on $ \mathbb{C} ^{n}$ with respect to the standard basis on $ \mathbb{C} ^{n}, \left\{ \vec{ e_1} , \ldots \vec{ e_n}  \right\} $ is $ A$ where $ A_{ij} = \langle  \vec{ e_i} , \vec{ e_j}   \rangle = \begin{cases}
      0 & \text{if } i\neq j \\
      1 &  \text{if } i=j
     \end{cases}
    \qquad  \implies A = I_n
    $
   }
   As before, with the inner product defined on $ \mathbb{C} ^{n}$, we can define length, $ \forall  \vec{ x} \in \mathbb{C} ^{n}$
   \[
   \| \vec{ x} \|= \sqrt{ \langle \vec{ x} ,\vec{ x}   \rangle } 
   .\] 
   and, the angle between two vectors $ \vec{ x} , \vec{ y} \in \mathbb{C} ^{n}$ as $ \theta $ where $ \langle \vec{ x} ,\vec{ y}   \rangle = \| \vec{ x} \| \| \vec{ y} \| \cos \theta$
   \\
   \mlem{}{
    Suppose $ \langle ,  \rangle : \mathbb{C} ^{n }  \times  \mathbb{C} ^{n} \to \mathbb{C}$   is the dot prodcut on $ \mathbb{C} ^{n}$. Then 
    \begin{enumerate}[label=(\roman*)]
      \item $ \langle \vec{ x} , A \vec{ y}   \rangle = \langle A ^{*} \vec{ x} , \vec{ y}   \rangle $ and 
      \item   $ \langle A \vec{ x} , \vec{ y}   \rangle = \langle \vec{ x} , A ^{*} \vec{ y}   \rangle $
      \end{enumerate}
   }
   \pf{Proof:}{
    \begin{align*}
     \langle \vec{ x} , A \vec{ y}   \rangle &= \left( \vec{ x}  \right) ^{*} A \vec{ y} = \overline{ \left( \vec{ x}  \right) ^{T}} A \vec{ y} \\
     &= \overline{ \left( \vec{ x} ^{T} \right) \left( A ^{T} \right) ^{ T} \vec{ y} \\
      &= \overline{ \left( \vec{ x}  \right) ^{T}} \overline{ \overline{ \left( A^{T} \right) ^{T}}} \vec{ y} \\
      &= \overline{ \left( \vec{ x}  \right) ^{T}} \overline{ \left(  \overline{ \left(  A ^{T} \right) } \right) ^{ T}} \vec{ y } \qquad  \text{ note }  \overline{ B ^{T}} = \overline{ B} ^{T}\\
      &= \overline{ \left( \vec{ x}  \right) ^{T} \left(  \overline{\left( A^{T} \right) } \right) ^{T} } \vec{ y} \\
      &= \left( A ^{*} \vec{ x}  \right) ^{*} \vec{ y} \\
      &= \langle A ^{*} \vec{ x} , \vec{ y}   \rangle
    .\end{align*}
    (ii )  can be shown in a similar manner.\\
   }
   \nt{
   These formulas also hold for the standard inner product on $ \mathbb{R} ^{n}$,\\
   \[
   \langle ,  \rangle : \mathbb{R} ^{n} \times  \mathbb{R} ^{n} \to \mathbb{R}
   .\] 
   \begin{enumerate}[label=(\roman*)]
     \item $ \langle \vec{ x} , A \vec{ y}   \rangle = \langle A ^{T} \vec{ x} , \vec{ y}   \rangle $ 
     \item $ \langle A \vec{ x} , \vec{ y}   \rangle = \langle \vec{ x} , A ^{T} \vec{ y}   \rangle $
     \end{enumerate}
     Same proof as above without complex conjugation.\\
   }
   \thm{}
   {
   Suppose $ A \in M _{ n \times  n} \left( \mathbb{C} \right) $. If $ A ^{*}= A$ ( i.e. $ A$ is Hermitian)  then all eigenvalues of $ A$ are real.\\
   }

   \pf{Proof:}{
   Suppose $ \lambda$ is an eigenvalue of $ A$ with corresponding eigenvector $ \vec{ v} $, i.e.  $ A \vec{ v} = \lambda \vec{ v}  $ \\
   \begin{align*}
    \lambda \langle \vec{ v} , \vec{ v}   \rangle   &= \langle \vec{ v} , \lambda \vec{ v}  \rangle = \langle \vec{ v} , A \vec{ v}   \rangle \\
    &= \langle A ^{*} \vec{ v} , \vec{ v}   \rangle = \langle A \vec{ v} , \vec{ v}   \rangle = \langle \lambda \vec{ v} , \vec{ v}   \rangle\\
    &\implies \lambda \langle \vec{ v} ,\vec{ v}   \rangle = \overline{ \lambda} \langle \vec{ v} ,\vec{ v}   \rangle\\
   .\end{align*}
   Hence $ \lambda = \overline{\lambda}$  i.e.  $  \lambda \in \mathbb{R}$
   }
   \nt{
   The theorem above also holds if $ A$ is a real symmetric matrix, i.e.  $ A ^{T} = A$.\\
   Since $ A$ determines a hermitian sesquilinear form of $ \mathbb{C} ^{n}$ , using the same proof as above, we get that $ \lambda = \overline{\lambda}$ for all eigenvalues $ \lambda $ of $ A$ i.e.  all eigenvalues of $ A$ are real.\\
   }
   \ex{}{
   \[
   A = \begin{bmatrix}
   1 & 2\\
   2 & 4\\
   \end{bmatrix}
   .\] 
   Eigenvalues\\
   \begin{align*}
    \left( 1-\lambda \right) \left( 4 - \lambda \right) &= 0\\
    \lambda^2 - 5 \lambda = 0\\
    \lambda \left( \lambda - 5 \right) &= 0\\
    \lambda_1 &= 0, \lambda_2 = 5\\
   .\end{align*}
   \[
   \lambda=0 \to \vec{ v_1} = \begin{bmatrix}
   -2\\
   1\\
   \end{bmatrix}
   \qquad  \lambda=5 \to \vec{ v_2} = \begin{bmatrix}
   1\\
   2\\
   \end{bmatrix}
   .\] 
   }
   \thm{Orthogonal Eigenvectors :}
   {
   Suppose $ A ^{*} = A$. Suppose $ A \vec{ v_1} = \lambda_1 \vec{ v_1} $ and $ A \vec{ v_2} = \lambda_2 \vec{ v_2} $ with $  \lambda_1 \neq \lambda_2$.\\
   Then $ \langle \vec{ v_1} , \vec{ v_2}   \rangle = 0$ i.e.  $ \vec{ v_1}$ and $ \vec{ v_2}$ are orthogonal.\\
   }

   \pf{Proof:}{
     \begin{align*}
      \lambda_2 \langle \vec{ v_1} , \vec{ v_2}   \rangle &= \langle \vec{ v_1} , A \vec{ v_2}   \rangle \\
      &= \langle A ^{*} \vec{ v_1} , \vec{ v_2}   \rangle \\
      &= \langle A \vec{ v_1} , \vec{ v_2}   \rangle \\
      &= \langle \lambda_1 \vec{ v_1} , \vec{ v_2}   \rangle \\
      &=  \overline{\lambda_1} \langle \vec{ v_1} , \vec{ v_2}   \rangle \qquad  \text{ since } \lambda_1 , \lambda_2 \in \mathbb{R}\\
      \implies \left( \lambda_2 - \lambda_1 \right) \langle \vec{ v_1} , \vec{ v_2}   \rangle &= 0\\
      \text{ since } \lambda_2 \neq \lambda_1 \implies \langle \vec{ v_1} , \vec{ v_2}   \rangle = 0
     .\end{align*}

   }
   Corollary: \\
   If $ A$ is a real symmetric matrix the eigenvectors corresponding to distinct eigenvalues are orthogonal.\\
   \ex{}{
   \[
   A = \begin{bmatrix}
   1 & 2 & 1\\
   2 & 0 & 2\\
   1 & 2 & 1\\
   \end{bmatrix}
   .\] 
   Find $ B$ such that $ B ^{T}A B = \begin{bmatrix}
   0 & 0 & 0\\
   0 & 4 & 0\\
   0 & 0 & -2\\
   \end{bmatrix}$\\
   \\
   \[
   B = \begin{bmatrix}
   - \frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{3} }  & \frac{1}{ \sqrt{6} }\\
   0 & \frac{1}{\sqrt{3} } & - \frac{ 2  }{ \sqrt{6}  } \\
   \frac{1}{ \sqrt{2} }& \frac{1}{\sqrt{3} } &  \frac{1}{ \sqrt{6} }\\
   \end{bmatrix}  \qquad  \left( B ^{T}A B \right) _{ ij} =0 \forall  i \neq j
   .\] 
   \[
   \left( B ^{T}A B \right) _{ ii} = \lambda_i \left( \| \vec{ v_i} \| ^2 \right)  = \lambda_i
   .\] 
   The eigenvalues of $ A$ are $ 0, 4, -2$ with corresponding eigenvectors \[
   \begin{bmatrix}
   -1\\
   0\\
   1\\
   \end{bmatrix}
    , \qquad  \begin{bmatrix}
    1\\
    1\\
    1\\
    \end{bmatrix}
     , \qquad  \begin{bmatrix}
     1\\
     -2\\
     1\\
     \end{bmatrix}
   .\]
   The eigenvectors corresponding to distinct eigenvalues are linearly independent and orthogonal, and hence the three of them form an orthogonal basis for $ \mathbb{R} ^{3}$.\\
   An orthonormal basis is given by:
   \[
   \mathcal{F} = \left\{ \frac{1}{ \sqrt{2} }  \begin{bmatrix}
   -1\\
   0\\
   1\\
   \end{bmatrix}
    . \frac{1}{ \sqrt{3} } \begin{bmatrix}
    1\\
    1\\
    1\\
    \end{bmatrix}
     , \frac{1}{ \sqrt{6} }  \begin{bmatrix}
     1\\
     -2\\
     1\\
     \end{bmatrix}  \right\} 
   .\] 
   }

   \ex{}{
   \[
   A = \begin{bmatrix}
   3 & 2\\
   2 & 6\\
   \end{bmatrix}
   .\] 
   Find $ B$ such that $ B ^{T}A B $ is a diagonal matrix.\\
   Eigenvalues:\\
   \begin{align*}
   \left( 3 - \lambda \right) \left( 6 - \lambda \right) - 4 &= 0\\
   \lambda^2 - 9 \lambda + 14 &= 0\\
   \left( \lambda-7 \right) \left( \lambda - 2 \right) &= 0\\
   \lambda_1 &= 7, \lambda_2 = 2\\
  \text{ orthogonal eigenvectors} \vec{ v_1} &= \begin{bmatrix}
   1\\
   2\\
   \end{bmatrix}
    \qquad  \vec{ v_2} = \begin{bmatrix}
    2\\
    -1\\
    \end{bmatrix}\\
   \text{ orthonormal eigenvectors} \frac{1}{ \sqrt{5} }\begin{bmatrix}
   1\\
   2\\
   \end{bmatrix}
   , \qquad  \frac{1}{ \sqrt{5} } \begin{bmatrix}
   2\\
   -1\\
   \end{bmatrix}
   .\end{align*}
   Let 
   \begin{align*}
    B &= \frac{1}{ \sqrt{5} } \begin{bmatrix}
    1 & 2\\
    2 & -1\\
    \end{bmatrix}\\
    B ^{T} &= \frac{1}{ \sqrt{5} } \begin{bmatrix}
    1 & 2\\
    2 & -1\\
    \end{bmatrix}\\
    B ^{T}A B &= \begin{bmatrix}
    7 & 0\\
    0 & 2\\
    \end{bmatrix}
   .\end{align*}
   }
   \ex{}{
   Let $ A = \begin{bmatrix}
   0 & 1 & 1\\
   1 & 0 & 1\\
   1 & 1 & 0\\
   \end{bmatrix}$ Find $ B$ such that $ B ^{T}A B$ is a diagonal matrix.\\
   \begin{align*}
    \chi _A \left( \lambda \right) &= - \lambda \left( \lambda ^2 -1 \right) -1 \left( -\lambda -1 \right) +1 \left( 1+\lambda \right) \\
    &= - \lambda \left( \lambda+1 \right) \left( \lambda-1 \right) + \left( 1+\lambda \right) \\
    &= \left( \lambda+1 \right) \left( - \lambda ^2 + \lambda +2 \right) \\
    &= \left( -1 \right) \left( \lambda+1 \right) \left( \lambda-2 \right) \left( \lambda+1 \right) \\
    &= - \left( \lambda+1 \right) ^2 \left( \lambda-2 \right)\\
   .\end{align*}
   \[
   \lambda = -1: \mathcal{N} \left( A +I \right) : \left[
   \begin{array}{ccc;{2pt/2pt}c}  
     1 & 1 & 1 & 0\\
     1 & 1 & 1 & 0\\
     1 & 1 & 1 & 0\\
   \end{array}
   \right] \to \left[
   \begin{array}{ccc;{2pt/2pt}c}  
     1 & 1 & 1 & 0\\
     0 & 0 & 0 & 0\\
     0 & 0 & 0 & 0 \\
   \end{array}
   \right]
   .\] 
   \[
   dim \mathcal{N} \left( A +I \right) =  2
   .\] 
   \[
   \text{ Let } \vec{ v_1} = \begin{bmatrix}
   1\\
   0\\
   -1\\
   \end{bmatrix}
     \qquad  \vec{ v_2} =  \begin{bmatrix}
     1\\
     -1\\
     0\\
     \end{bmatrix}
   .\] 
   \[
   \lambda=2 : \mathcal{N} \left( A -2I \right) : \left[
   \begin{array}{ccc;{2pt/2pt}c}  
     -2 & 1 & 1 & 0\\
     1  & -2  & 1 & 0\\
     1 & 1 & -2 & 0\\
   \end{array}
   \right]            \to \left[
   \begin{array}{ccc;{2pt/2pt}c}  
     1 & 1 & -2 & 0\\
     0 & -3 & 3 & 0\\
     0 & 3 & -3 & 0\\
   \end{array}
   \right]   \to \left[
   \begin{array}{ccc;{2pt/2pt}c}  
     1 & 0 & -1 & 0\\
     0 & 1 & -1 & 0\\
     0 & 0 & 0 & 0\\
   \end{array}
   \right]
   .\]
   $ x = z \qquad  y = z$\\
   Let $ \vec{ v_3} = \begin{bmatrix}
   1\\
   1\\
   1\\
   \end{bmatrix}
    $ \\
    Note: $ \langle \vec{v_1 }, \vec{ v_3}   \rangle =0 $, $ \qquad  \langle \vec{ v_2} , \vec{ v_3}   \rangle =0$ but $ \langle \vec{ v_1} , \vec{ v_2}   \rangle  \neq 0$ \\
    $ \implies$ Apply Gram-Schmidt process to $ \vec{ v_1} , \vec{ v_2}$ to orthogonalize them.\\
    Let $ \vec{ w_1} = \vec{ v_1} $\\
    \[
    \vec{ w_2} = \vec{ v_2} - \frac{ \langle \vec{ v_2} , \vec{ w_1}   \rangle }{ \langle \vec{ w_1} , \vec{ w_1}   \rangle } \vec{ w_1} 
    .\] 
    \[
    \vec{ w_2} = \begin{bmatrix}
    1\\
    -1\\
    0\\
    \end{bmatrix}
     = \frac{1}{2} \begin{bmatrix}
     1\\
     0\\
     -1\\
     \end{bmatrix}
      =  \begin{bmatrix}
       \frac{1}{2}\\
      -1\\
      \frac{1}{2}\\
      \end{bmatrix}
    .\] 
    \[
    \text{ Orthonormal basis } \mathcal{F} = \left\{ \frac{1}{ \sqrt{2} }  \begin{bmatrix}
    1\\
    0\\
    -1\\
    \end{bmatrix}
     , \frac{1}{ \sqrt{6} } \begin{bmatrix}
     1\\
     -2\\
     1\\
     \end{bmatrix}
      , \frac{1}{ \sqrt{3} } \begin{bmatrix}
      1\\
      1\\
      1\\
      \end{bmatrix}
    \right\}
    .\] 
    Let $ B = \begin{bmatrix}
    \frac{1}{ \sqrt{2} } & \frac{1}{ \sqrt{6} } & \frac{1}{ \sqrt{3} }\\
     0 & - \frac{ 2  }{  \sqrt{6}  } & \frac{1}{ \sqrt{3} } \\
     - \frac{1}{ \sqrt{2} }& \frac{1}{ \sqrt{6} }  & \frac{1}{ \sqrt{3} } \\
    \end{bmatrix}$
    \[
    B ^{T}A B = \begin{bmatrix}
    -1 & 0 & 0\\
    0 & -1 & 0\\
    0 & 0 & 2\\
    \end{bmatrix}
    .\] 
   }
   \section{Orthogonal Matrices}
   \dfn{Orthogonal Matrix :}{
     Suppose $ B \in M_{n \times  n}$. $ B$ is said to be \underline{orthogonal} if $ B ^{T} = B ^{-1}$ or $ B^{T}B = I_n$
   }
   \thm{}
   {
   $ B \in M _{ n \times  n}\left( \mathbb{R} \right) $ is orthogonal if and only if the columns of $ B$ are orthonormal 
   }
   \pf{Proof:}{
   $ B ^{T}B = I_n$ \\
   $ \iff \left( B ^{T} B \right) _{ ij} = begin{cases}
    1 & \text{if } i =j \\
    0 & \text{ if }  i \neq j
   \end{cases}
   $    
   \[
    \iff \left( i ^{ \text{th}} \text{row of } B ^{T} \right) \cdot  \left( j ^{ \text{ th}} \text{ column of } B\right)  = begin{cases}
     1 & \text{if } i= j \\
     0 & \text{if } i \neq j
    \end{cases}
    
   .\] 
   \[
    \iff \left( i ^{ \text{ th}} \text{ column of } B \right)  \cdot  \left( j ^{ \text{ th}} \text{ column of } B \right) = begin{cases}
     1 & \text{if } i = j \\
     0 & \text{if } i \neq j
    \end{cases}
    
   .\] 
   \[
    \iff \text{ The columns of } B \text{ are orthonormal.}
   .\] 
   }
   
   \underline{Fact:}\\
   The determinant of an orthogonal matrix $ A \in M _{ n \times  n} \left( \mathbb{R} \right) $ is either $ 1$ or $ -1$.\\
   \pf{Proof:}{
    \[
    A ^{T} A = I_n 
    .\] 
    \[
    \implies \text{ det } \left( A^{T}A \right) = \text{ det } I_n 
    .\]
    \[
    \text{ det } \left( A^{T} \right) \text{ det } A = 1
    .\] 
    \[
    \implies \left(  \text{ det } A \right) ^2 =1 \text{ since } \text{ det } A = \text{ det } \left( A^{T} \right)
    .\]
    \[
    \implies \text{ det } A = \pm 1
    .\] 
   }
   
   \mlem{}{
   Orthogonal matrices preserve the dot product on $  \mathbb{R} ^{n}$
  }
   \pf{Proof:}{
    Suppose $ A$ is an orthogonal matrix, $ \vec{ x} , \vec{ y} \in \mathbb{R} ^{n}$. $ \vec{ x} , \vec{ y } \in \mathbb{R} ^{n}$, $ \langle A \vec{ x} , A \vec{ y}   \rangle = \langle A ^{T} A \vec{ x} , \vec{ y}   \rangle = \langle \vec{ x} , \vec{ y}   \rangle $\\
    In particular, 
    \begin{align*}
     \langle A \vec{ x} , A \vec{ x}   \rangle &= \langle  \vec{ x}  , \vec{ x}  \rangle \\
     \|A \vec{ x} \|^2 &= \| \vec{ x} \| ^2\\
     \implies \| A \vec{ x} \| &= \| \vec{ x} \|\\
    .\end{align*}
   }
   
   
   
    
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
    
         
         
         
          
         
         

















\end{document}  
